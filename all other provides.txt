import logging
from typing import List, Optional

import backoff
from anthropic import AsyncAnthropic

from ..llm import BaseLLMProvider, LLMResponse, Message, ThinkingBlock
from ..providers.anthropic_bedrock import AnthropicBedrockProvider

logger = logging.getLogger(__name__)


class AnthropicProvider(BaseLLMProvider):
	def __init__(self, model: str, enable_thinking: bool = True, thinking_token_budget: Optional[int] = 2048):
		super().__init__(model=model)
		self.client = AsyncAnthropic()
		self.thinking_token_budget = thinking_token_budget

		self.anthropic_bedrock = AnthropicBedrockProvider(model=f"us.anthropic.{model}-v1:0", enable_thinking=enable_thinking, thinking_token_budget=thinking_token_budget)

		self.enable_thinking = enable_thinking

	@backoff.on_exception(
		backoff.constant,  # constant backoff
		Exception,     # retry on any exception
		max_tries=3,   # stop after 3 attempts
		interval=10,
		on_backoff=lambda details: logger.info(
			f"API error, retrying in {details['wait']:.2f} seconds... (attempt {details['tries']})"
		)
	)
	async def call(
		self,
		messages: List[Message],
		temperature: float = -1,
		max_tokens: Optional[int] = 16000,
		**kwargs
	) -> LLMResponse:
		# Make a copy of messages to prevent modifying the original list during retries
		messages_copy = messages.copy()

		if not messages_copy:
			raise ValueError("Messages list cannot be empty.")

		conversation_messages_input: List[Message] = []

		system = []

		if messages_copy[0].role == "system":
			system = messages_copy[0].content[0].text
			conversation_messages_input = messages_copy[1:]
		else:
			conversation_messages_input = messages_copy

		anthropic_api_messages = [msg.to_anthropic_format() for msg in conversation_messages_input]

		if self.enable_thinking:

			try:
				response = await self.client.messages.create(
					model=self.model,
					system=system,
					messages=anthropic_api_messages,
					thinking={
						"type": "enabled",
						"budget_tokens": self.thinking_token_budget,
					},
					max_tokens=max(self.thinking_token_budget + 1, max_tokens),
					**kwargs
				)
			except Exception as e:
				logger.error(f"Error calling Anthropic: {str(e)}")
				# Fallback to anthropic_bedrock with the original messages_copy
				response = await self.anthropic_bedrock.call(
					messages_copy, # Pass original messages_copy, bedrock provider has its own logic
					temperature=temperature, # Pass original temperature
					max_tokens=max_tokens,   # Pass original max_tokens
					**kwargs
				)

			return LLMResponse(
				content=response.content[1].text,
				raw_response=response,
				usage=response.usage.model_dump(),
				thinking=ThinkingBlock(thinking=response.content[0].thinking, signature=response.content[0].signature)
			)
		else: # Not enable_thinking
			response = await self.client.messages.create(
				model=self.model,
				messages=anthropic_api_messages,
				temperature=temperature, # Use adjusted temperature
				max_tokens=max_tokens, # Use adjusted max_tokens
				system=system,
				**kwargs
			)

			return LLMResponse(
				content=response.content[0].text,
				raw_response=response,
				usage=response.usage.model_dump()
			)
			
			
			import logging
			import os
			from typing import List, Optional
			
			import backoff
			from anthropic import AsyncAnthropicBedrock
			from dotenv import load_dotenv
			
			from ..llm import BaseLLMProvider, LLMResponse, Message
			
			load_dotenv()
			
			logger = logging.getLogger(__name__)
			
			
			class AnthropicBedrockProvider(BaseLLMProvider):
				def __init__(self, model: str, enable_thinking: bool = True, thinking_token_budget: Optional[int] = 8192):
					super().__init__(model=model)
			
					self.client = AsyncAnthropicBedrock(
						aws_access_key=os.getenv('AWS_ACCESS_KEY_ID'),
						aws_secret_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
						aws_region=os.getenv('AWS_REGION'),
					)
					self.enable_thinking = enable_thinking
					self.thinking_token_budget = thinking_token_budget
				@backoff.on_exception(  # noqa: F821
					backoff.constant,  # constant backoff
					Exception,     # retry on any exception
					max_tries=3,   # stop after 3 attempts
					interval=10,
				)
				async def call(
					self,
					messages: List[Message],
					temperature: float = 1,
					max_tokens: Optional[int] = 2048,
					**kwargs
				) -> LLMResponse:
			
					messages_copy = messages.copy()
			
					if len(messages_copy) < 2 or messages_copy[0].role != "system":
						raise ValueError("System message is required for Anthropic Bedrock and length of messages must be at least 2")
			
					system_message = messages_copy[0]
			
					try:
						if self.enable_thinking:
			
							response = await self.client.messages.create(
								model=self.model,
								system=system_message.to_anthropic_format(enable_cache_control=False)["content"],
								messages=[msg.to_anthropic_format(enable_cache_control=False) for msg in messages_copy[1:]],
								temperature=1,
								thinking={
									"type": "enabled",
									"budget_tokens": self.thinking_token_budget,
								},
								max_tokens=max(self.thinking_token_budget + 1, max_tokens),
								**kwargs
							)
			
							return LLMResponse(
								content=response.content[1].text,
								raw_response=response,
								usage=response.usage
							)
						else:
			
							response = await self.client.messages.create(
								model=self.model,
								messages=[msg.to_anthropic_format(enable_cache_control=False) for msg in messages_copy[1:]],
								temperature=temperature,
								max_tokens=max_tokens,
								system=system_message.to_anthropic_format(enable_cache_control=False)["content"],
								**kwargs
							)
			
							return LLMResponse(
								content=response.content[0].text,
								raw_response=response,
								usage=response.usage
							)
					except Exception as e:
						logger.error(f"Error calling Anthropic Bedrock: {str(e)}")
						raise e
						
						
						from typing import List, Optional
						
						from openai import AsyncOpenAI
						
						from ..llm import BaseLLMProvider, LLMResponse, Message
						
						
						class OpenAIProvider(BaseLLMProvider):
							def __init__(self, model: str, reasoning_effort: Optional[str] = "low"):
								super().__init__(model=model)
								self.client = AsyncOpenAI()
								self.reasoning_effort = reasoning_effort
						
							async def call(
								self,
								messages: List[Message],
								temperature: float = 1.0,
							) -> LLMResponse:
						
								args = {
									"temperature": temperature,
								}
						
								if self.model.startswith("o") and self.reasoning_effort:
									args["reasoning_effort"] = self.reasoning_effort
									args["temperature"] = 1
						
								response = await self.client.chat.completions.create(
									model=self.model,
									messages=[msg.to_openai_format() for msg in messages],
									**args
								)
						
								return LLMResponse(
									content=response.choices[0].message.content,
									raw_response=response,
									usage={
										"prompt_tokens": response.usage.prompt_tokens,
										"completion_tokens": response.usage.completion_tokens,
										"total_tokens": response.usage.total_tokens
									}
								) 
								
								
								import logging
								import os
								from typing import List, Optional
								
								import backoff
								from google import genai
								
								from ..llm import BaseLLMProvider, LLMResponse, Message
								
								logger = logging.getLogger(__name__)
								class GeminiProvider(BaseLLMProvider):
									def __init__(self, model: str, thinking_token_budget: int = 8192):
										super().__init__(model=model)
										self.client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
										self.thinking_token_budget = thinking_token_budget
								
								
									@backoff.on_exception(
										backoff.constant,  # constant backoff
										Exception,     # retry on any exception
										max_tries=3,   # stop after 3 attempts
										interval=0.5,
										on_backoff=lambda details: logger.info(
											f"API error, retrying in {details['wait']:.2f} seconds... (attempt {details['tries']})"
										),
									)
									async def call(
										self,
										messages: List[Message],
										temperature: float = 1.0,
										max_tokens: Optional[int] = None,
										**kwargs
									) -> LLMResponse:
								
										if len(messages) == 0:
											raise ValueError("Messages must be non-empty")
								
										config = {
											"temperature": temperature,
											"thinking_config": {
												"thinking_budget": self.thinking_token_budget
											},
										}
								
										if messages[0].role == "system":
											system = messages[0].content[0].text
											gemini_messages = [msg.to_gemini_format() for msg in messages[1:]]
								
											config["system_instruction"] = {
												"text": system
											}
										else:
											gemini_messages = [msg.to_gemini_format() for msg in messages]
								
								
										if max_tokens:
											config["max_output_tokens"] = max_tokens
								
										response = await self.client.aio.models.generate_content(
											model=self.model,
											contents=gemini_messages,
											config=config,   
										)
								
										# Extract usage information if available
										usage = {}
										if hasattr(response, "usage_metadata"):
											usage = {
												"prompt_tokens": getattr(response.usage_metadata, "prompt_token_count", 0),
												"completion_tokens": getattr(response.usage_metadata, "candidates_token_count", 0),
												"total_tokens": getattr(response.usage_metadata, "total_token_count", 0)
											}
								
										return LLMResponse(
											content=response.text,
											raw_response=response,
											usage=usage
										) 